{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Qwen3 14B Runner with Ngrok\n",
    "\n",
    "Este notebook irá:\n",
    "1. Instalar o Ollama.\n",
    "2. Instalar o Ngrok.\n",
    "3. Configurar o token de autenticação do Ngrok (espera-se que esteja definido como um segredo do Kaggle chamado `NGROK_AUTHTOKEN`).\n",
    "4. Baixar o modelo `qwen3:14b` do Ollama.\n",
    "5. Iniciar o servidor Ollama em segundo plano.\n",
    "6. Expor a porta do Ollama (11434) usando Ngrok.\n",
    "7. Salvar a URL pública do Ngrok em `ngrok_url.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# --- Configurações ---\n",
    "OLLAMA_MODEL = \"qwen3:14b\"\n",
    "NGROK_LOG_FILE = \"ngrok.log\"\n",
    "OUTPUT_URL_FILE = \"/kaggle/working/ngrok_url.txt\" # O GitHub Actions espera isso em /kaggle/output, mas o kernel salva em /kaggle/working primeiro
",
    "OLLAMA_PORT = 11434\n",
    "\n",
    "print(\"Passo 1: Tentando obter o NGROK_AUTHTOKEN dos segredos do Kaggle\")\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    NGROK_TOKEN = user_secrets.get_secret(\"NGROK_AUTHTOKEN_USER\") # Deve corresponder ao nome do segredo no Kaggle
",
    "    if not NGROK_TOKEN:\n",
    "        print(\"ERRO: NGROK_AUTHTOKEN_USER não encontrado ou vazio nos segredos do Kaggle.\")\n",
    "        print(\"Por favor, adicione seu token Ngrok como um segredo chamado 'NGROK_AUTHTOKEN_USER' nas configurações deste notebook Kaggle.\")\n",
    "        exit()\n",
    "except ImportError:\n",
    "    print(\"ERRO: kaggle_secrets não pôde ser importado. Certifique-se de que está executando em um ambiente Kaggle com segredos habilitados.\")\n",
    "    print(\"Se estiver testando localmente, defina NGROK_TOKEN manualmente ou via variável de ambiente.\")\n",
    "    # Para testes locais, você pode descomentar e definir o token abaixo, mas NÃO FAÇA COMMIT COM SEU TOKEN REAL!
",
    "    # NGROK_TOKEN = \"SEU_TOKEN_NGROK_AQUI_PARA_TESTES_LOCAIS\" 
",
    "    if 'NGROK_TOKEN' not in locals():
",
    "       exit()
",
    "print(\"NGROK_AUTHTOKEN obtido com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Passo 2: Instalando Ollama...\")\n",
    "# Instala Ollama
",
    "subprocess.run([\"curl\", \"-fsSL\", \"https://ollama.com/install.sh\", \"|\", \"sh\"], check=True, shell=True, capture_output=True, text=True)\n",
    "print(\"Ollama instalado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Passo 3: Baixando e instalando Ngrok...\")\n",
    "subprocess.run([\"wget\", \"https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\"], check=True, capture_output=True)\n",
    "subprocess.run([\"tar\", \"xvzf\", \"ngrok-v3-stable-linux-amd64.tgz\"], check=True, capture_output=True)\n",
    "subprocess.run([\"chmod\", \"+x\", \"ngrok\"], check=True)\n",
    "subprocess.run([\"./ngrok\", \"config\", \"add-authtoken\", NGROK_TOKEN], check=True, capture_output=True)\n",
    "print(\"Ngrok instalado e autenticado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Passo 4: Iniciando o servidor Ollama em segundo plano na porta {OLLAMA_PORT}...\")\n",
    "# Inicia o servidor Ollama em segundo plano
",
    "# Definir explicitamente CUDA_VISIBLE_DEVICES pode ser necessário em alguns ambientes Kaggle GPU
",
    "ollama_env = os.environ.copy()\n",
    "# ollama_env[\"OLLAMA_HOST\"] = f\"0.0.0.0:{OLLAMA_PORT}\" # Ollama 0.1.30+ usa OLLAMA_HOST para porta também
",
    "# ollama_env[\"OLLAMA_PORT\"] = str(OLLAMA_PORT) # Para versões mais antigas
",
    "ollama_process = subprocess.Popen([\"/usr/local/bin/ollama\", \"serve\"], env=ollama_env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"Servidor Ollama iniciado. PID:\", ollama_process.pid)\n",
    "\n",
    "# Espera um pouco para o servidor Ollama iniciar
",
    "time.sleep(15)\n",
    "\n",
    "# Verifica se o servidor está rodando
",
    "try:\n",
    "    response = requests.get(f\"http://127.0.0.1:{OLLAMA_PORT}/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"Servidor Ollama está respondendo.\")\n",
    "    else:\n",
    "        print(f\"Falha ao verificar o servidor Ollama. Status: {response.status_code}. Resposta: {response.text}\")\n",
    "        print(\"Logs do Ollama (stdout):\")\n",
    "        # stdout_data, stderr_data = ollama_process.communicate(timeout=5) # Não usar communicate aqui pois bloqueia
",
    "        # print(stdout_data.decode() if stdout_data else \"Nenhum stdout\")
",
    "        # print(\"Logs do Ollama (stderr):\")
",
    "        # print(stderr_data.decode() if stderr_data else \"Nenhum stderr\")
",
    "        # exit(1)
",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"Erro de conexão ao verificar o servidor Ollama: {e}\")\n",
    "    print(\"Verifique se o Ollama iniciou corretamente. Pode ser necessário mais tempo ou verificar logs.\")\n",
    "    # exit(1)
",
    "print(\"Continuando com o download do modelo...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Passo 5: Baixando o modelo {OLLAMA_MODEL} com Ollama... Isso pode levar algum tempo.\")\n",
    "# Baixa o modelo especificado
",
    "pull_command = [\"/usr/local/bin/ollama\", \"pull\", OLLAMA_MODEL]\n",
    "pull_process = subprocess.Popen(pull_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "while True:\n",
    "    output = pull_process.stdout.readline()\n",
    "    if output == '' and pull_process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.strip()) # Imprime o progresso do download
",
    "\n",
    "pull_process.wait()\n",
    "\n",
    "if pull_process.returncode == 0:\n",
    "    print(f\"Modelo {OLLAMA_MODEL} baixado com sucesso.\")\n",
    "else:\n",
    "    print(f\"Falha ao baixar o modelo {OLLAMA_MODEL}.\")\n",
    "    error_output = pull_process.stderr.read()\n",
    "    print(f\"Erro: {error_output}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Passo 6: Expondo a porta {OLLAMA_PORT} com Ngrok...\")\n",
    "# Inicia Ngrok para expor a porta do Ollama
",
    "ngrok_process = subprocess.Popen([\"./ngrok\", \"http\", str(OLLAMA_PORT), \"--log\", NGROK_LOG_FILE], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print(\"Ngrok iniciado. PID:\", ngrok_process.pid)
",
    "\n",
    "# Espera um pouco para o Ngrok iniciar e gerar a URL
",
    "time.sleep(10) # Aumentado para 10 segundos
",
    "\n",
    "# Obtém a URL pública do Ngrok através da API local do Ngrok
",
    "try:\n",
    "    response = requests.get(\"http://127.0.0.1:4040/api/tunnels\") # API do cliente Ngrok
",
    "    response.raise_for_status() # Levanta um erro para códigos de status HTTP ruins (4xx ou 5xx)
",
    "    tunnels_data = response.json()\n",
    "    \n",
    "    public_url = None\n",
    "    if tunnels_data and \"tunnels\" in tunnels_data and len(tunnels_data[\"tunnels\"]) > 0:\n",
    "        # Prioriza túneis https, se disponíveis
",
    "        for tunnel in tunnels_data[\"tunnels\"]:\n",
    "            if tunnel[\"proto\"] == \"https\":
",
    "                public_url = tunnel[\"public_url\"]
",
    "                break\n",
    "        if not public_url: # Se não houver https, pega o primeiro túnel http
",
    "             public_url = tunnels_data[\"tunnels\"][0][\"public_url\"]
",
    "\n",
    "    if public_url:\n",
    "        print(f\"URL pública do Ngrok: {public_url}\")\n",
    "        with open(OUTPUT_URL_FILE, \"w\") as f:\n",
    "            f.write(public_url)\n",
    "        print(f\"URL salva em {OUTPUT_URL_FILE}\")\n",
    "    else:\n",
    "        print(\"ERRO: Não foi possível obter a URL pública do Ngrok da API.\")\n",
    "        print(\"Dados dos túneis:\", tunnels_data)\n",
    "        # Tenta ler o log do ngrok como fallback
",
    "        if os.path.exists(NGROK_LOG_FILE):\n",
    "            with open(NGROK_LOG_FILE, 'r') as log_f:\n",
    "                log_content = log_f.read()\n",
    "            print(f\"Conteúdo do log do Ngrok ({NGROK_LOG_FILE}):\n{log_content}\")\n",
    "        exit(1)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"ERRO ao conectar à API do Ngrok: {e}\")\n",
    "    if os.path.exists(NGROK_LOG_FILE):\n",
    "        with open(NGROK_LOG_FILE, 'r') as log_f:\n",
    "            log_content = log_f.read()\n",
    "        print(f\"Conteúdo do log do Ngrok ({NGROK_LOG_FILE}):\n{log_content}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mantendo o Notebook Rodando\n",
    "\n",
    "O servidor Ollama e o Ngrok estão rodando em segundo plano.\n",
    "Este notebook precisa continuar em execução para manter o túnel Ngrok ativo.\n",
    "A URL do Ngrok foi salva e estará disponível como output do kernel do Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"O servidor Ollama e o túnel Ngrok devem estar ativos.\")\n",
    "print(f\"A URL do Ngrok ({public_url}) foi salva em {OUTPUT_URL_FILE}.\")\n",
    "print(\"Este notebook continuará rodando para manter os serviços ativos.\")\n",
    "print(\"Você pode parar a execução deste notebook para encerrar o servidor Ollama e o túnel Ngrok.\")\n",
    "\n",
    "# Loop para manter o notebook ativo
",
    "try:\n",
    "    while True:\n",
    "        time.sleep(60) # Mantém o kernel vivo
",
    "        # Poderia adicionar verificações de saúde aqui se necessário
",
    "        print(f\"Heartbeat: {time.ctime()}. Ollama e Ngrok ainda rodando.\")
",
    "except KeyboardInterrupt:\n",
    "    print(\"Notebook interrompido. Encerrando processos...\")\n",
    "finally:\n",
    "    print(\"Encerrando Ngrok...\")\n",
    "    if 'ngrok_process' in locals() and ngrok_process.poll() is None:\n",
    "        ngrok_process.terminate()\n",
    "        ngrok_process.wait()\n",
    "    print(\"Encerrando Ollama...\")\n",
    "    if 'ollama_process' in locals() and ollama_process.poll() is None:\n",
    "        ollama_process.terminate()\n",
    "        ollama_process.wait()\n",
    "    print(\"Processos encerrados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

